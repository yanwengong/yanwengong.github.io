---
layout: post
#标题配置
title:  0_week1_理论课
#时间配置
date:   2018-06-17 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
tag: bittiger
---

* content
{:toc}


## Probability

1. Probability density function (PDF)
  * PDF: x ~ P(X = x)
  * Used to describe the frequency of possible outcome x's occurrences of the
  total possible occurrences

2. Cumulative density function (CDF)
  * CDF: x ~ P(X <= x)

3. Practice 1: 2 dices, probability of at least one 4 shows up (11/36)
Practice 2: 2 dices, probability of the sum is 6 (5/36)

## Expectation

1. Measure what is the center of a random variable, $$ E(X) = \sum xp(x) $$

2. Sample mean is used to estimate the population mean. If the samples are selected
unbiased. E(sample mean) = population mean

3. Q: why always larger sample is better?

4. Practice: One person goes to Casino, spends $5 for a game - roll dice twice. If the sum
is 6, he wins $21; if not, he gets nothing.
  * Q1: Is the game lean to Casino or player? (hint: calculate the expectation)
  * Q2: If this person keeps playing until the first win, what the probability that he wins
  money? (45.016%)

## Variance

1. Variance ( $$ \sigma^2 $$) measures how random variable spreads
    * $$ Var(X) = E[(X - u)^2] = E[X^2] - E[X]^2 $$
    * standard deviation is the square root of variance

2. Sample variance($$ s^2 $$) and standard deviation (s)
    * $$ S^2 = \frac_{\sum_{i=1}(X_{i} - \bar{X})^2}{n - 1} $$
    * Why n - 1: The degree of freedom is n - 1 not n

## Covariance and correlation

1. Covariance is a measure of the joint variability of two random variables.
  * Positive value: the greater values of one variable mainly correspond with the greater
  values of the other variable (the variables tend to show similar behavior)
  * Negative value: the variables tend to show opposite behavior
  * The sign of the covariance indicates the tendency in the linear relationship between
  the variables. The magnitude of the covariance is not easy to interpret because it is not normalized
  and hence depends on the magnitudes of the variables. The normalized version of the covariance is the
  correlation coefficient.

2. Equations
  * Covariance: $$ cov(X,Y) = E[(X - E[X])(Y - E[Y])] $$
  * Correlation: $$ corr(X,Y) = \frac_{cov(X,Y)}{\sigma_{X}\sigma_{Y}} $$
  * Sample covariance: $$ cov(X, Y) = \frac_{1}{n - 1}\sum_{i = 1}^{n}(x_{1} - \bar{x})(y_{i} - \bar{y}) =
  \frac_{1}{n - 1}(\sum x_{i}y_{i} - n\bar{x}\bar{y}) $$
  * Sample correlation: $$ cor(X, Y) = \frac_{cov(X, Y)}{S_{x}S_{y}} $$

## Conditional probability and Bayes' theorem

1. Equation: $$ P(B|A) = \frac_{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^{c})P(B^{c})} $$

2. Practice: There is an unfair coin, which P(H) = 0.7, and another fair coin P(H) = 0.5.
If after 10 flips, see 8 head. What's the probability that the unfair coin is chose?
hint: use Bayes and the answer is 0.84.

## Important distribution

1. Bernoulli distribution is a special case of Binomial distribution with n = 1.
Equation: $$ P(X = x) = p^{x}(1-p)^{1-x} $$ x = 0 or 1
Expectation and variance: E(X) = p, Var(X) = p(1-p)

2. Binomial distribution is sum of N Bernoulli variables: $$ P(X = x) = C_{x}^{N}p^{x}(1-p^{N-x}) $$
Expectation and variance: E(X) = N*p, Var(X) = N*p(1-p)

3. Geometric distribution: probability of seeing first success with k independent trails, each
with probability p.
Equation: $$ Pr(X = k) = (1 - p)^{k-1}p $$

4. Negative binomial distribution: Probability of seeing rth failure with k success trails,
each with success probability p.
Equation: $$ Pr(X = k) = C_{k}^{k+r-1}p^{k}(1-p)^{r} $$

5. Poisson distribution: count data per time unit
Equation: $$ P(X = k) = \frac_{\lambda^{k}}{k!}e^{-\lambda}
Expectation and variance: $$ \lambda $$

6. Exponential distribution: In a poisson distribution, internal-arrival time follows exponential distribution.
PDF: $$ \lambda e^{-\lambda x} $$ CDF: $$ 1 - e^{-\lambda x} $$
Expectation: $$ E[X] = \frac_{1}{\lambda} $$ Variance: $$ Var[X] = \frac_{1}{\lambda^{2}}

7. Normal distribution and standard normal distribution are two important continuous distribution

8. Central limit theorem: if the sample size is super big, the sample mean should follow normal distribution
TO BE CONTINUED

9. Confidence intervals
  * Purpose: Say we have a random sample from a population. We want to know how well the sample statistic estimate
  the population. A confidence intervals addresses the issue because it provides a range of values which is likely
  to contain the population parameter of interest.

10. Causal inference
  * def: process of drawing a conclusion about causality
  * causal inference vs (statistical) inference: (statistical) inference is drawn from observation; causal inference
  cannot drawn from observation, but from experiment
  * Observation data cannot be used to derive causality because there are confounding factors. We need to design
  randomized experiment (A/B testing)
  * Design randomized experiments:
    * define the causal relationship to be explored, X->Y  (new UI decreases user interaction)
    * Define metric (Y) (number of posts per user per day)
    * Design randomized experiments (A/B test) (randomly separate samples into two groups, control: old UI,
    experiment: new UI)
    * Collect data and conduct hypothesis testing (compare the metrics using two samples t test)

## Hypothesis Test   

1. Definition of hypothesis testing: use sample of data to test an assumption regarding a population parameter, which could be
  * population mean, u    
  * difference in two population means, u1 - u2    
  * a population variance     
  * ratio of two population variances    
  * three or more means, u1, u2, u3, etc.     

2. Causality test: Correlation is not causality. Test whether winning Nobel prize is due to high
chocolate consumption: separate people from one country randomly into two group - one group increase
the consumption of chocolate and see whether more people in this group win Nobel prize.

3. One sample vs Two sample tests
  * one sample: one population, compare test statistic, e.g, sample mean, with a known number
  * two sample: compare two population means     

4. How to construct a hypothesis testing     
  * Define H0 and Ha    
  * Define which test statistics to be calculated (z or p-value)  
    * calculate z score and compare with critical values; if z > z*, reject H0; else do not reject
    H0
    * calculate p value and compare with type I error ($$ \alpha $$), if p is smaller, reject H0;
    else do not reject H0   

  5. Z test vs T test    
