---
layout: post
#标题配置
title:  1_week3_理论课
#时间配置
date:   2018-07-04 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
tag: bittiger
---

* content
{:toc}


## Collinearity

0. TODOs   
  * VIF calculation
  * derive the equation of ridge and LASSO

1. Definition
  * Collinearity are highly correlated predictors. If involved more than 2
  predictors, it called multicolinearity.
  * If have two collinearity predictors, $$ \beta_{1} $$ and $$ \beta_{2} $$ and $$ \beta_{1} + \gamma $$ and $$ \beta_{2} - \gamma $$ give the same
  prediction     

2. Identification   
  * can use variance inflation factor (VIF) to quantify the mylticollinearity issue    

## Regularization   

1. Ridge: add square of beta as penalty when minimize MSE, solution:
$$ min \sum_{i = 1}^{n}(y_{i} - \beta^{T}x_{i})^2 + \lambda\sum_{j=1}^{p}(\beta_{j})^2 $$
$$ \beta_{ridge} = (X^{T}X + \lambda I_{p})^{-1}X^{T}Y $$      

2. LASSO (least absolute shrinkage and selection operator): add absolute value of beta as penalty; can do feature selection:
$$ min \sum_{i = 1}^{n}(y_{i} - \beta^{T}x_{i})^2 + \lambda\sum_{j=1}^{p}|\beta_{j}| $$     

3. Elastic net: combination of both ridge and lasso:
$$ \lambda\sum_{j=1}^{p}((1 - \alpha)\beta_{j}^{2} + \alpha|\beta_{j}|) $$   

4. How to choose lambda   
  * Traditional way: plot all coefficients against multiple value of λ, and choose λ when coefficients are not rapidly changing. R function: library (MASS), lm.ridge(), lars()    
  * current standard practice is cross validation    

## Cross validation    

1. Used to optimize the model with limited sample size     

2. Leave one out: for sample size n, leave one sample out and use all the other samples as training data to build model, and test the model on the one sample left out, and evaluate the model through SSE. Choose another sample as training data and do this again. Sum over the k times SSE, and divided by k.     

3. K-fold cross validation: split data into complementary k part. Leave the last part as validation data and use the rest to train; one run will result in k-1 model and test these model on the validation data set    

4. cross validation can be used to find the best $$ \lambda $$ for regularization    

## Logistic regression     

1. 
