---
layout: post
#标题配置
title:  03_linRegress
#时间配置
date:   2018-05-20 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
#tag: machine_learning
---

* content
{:toc}


## Linear regression
* equation to calculate expected value: $$ hat(y)(x) = \thetax^{T} $$
* MSE is used to quantify the error: $$ MSE = \frac{1}{m}\sum_{j}(y^{j}-\thetax^{jT})^2 = \frac_{1}{m}(y^{T}-\thetaX^{T})(y^{T}-\thetaX^{T})^{T} $$  
* Good thetas can make MSE small, the the way to find good parameters is to find the theta at the bottom of cost "surface".   

## Linear regression: gradient descent&stochastic gradient descent

* To find the smallest J(\theta), we need to calculate the derivative $$ \frac{\partial J(\theta)}{\partial \theta} $$ If it is positive, J(\theta) is increasing; if it is negative, J(\theta) is decreasing. If there are multiple \theta, then imaging multi-dimension space composed of \theta, then calculate the gradient vector.   
* fake algorithm of gradient descent: 1. initialization (?); 2. set step size (\alpha, size of each move); 3. gradient direction; 4. check whether stopping condition is met after each move  
* equation to calculate gradient descent <img src="/Users/YvonneGong/Documents/2018_spring/machine_learning/screenshot/gradiant_descent.png">    

## Linear regression: direct minimization   
