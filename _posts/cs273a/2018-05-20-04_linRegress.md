---
layout: post
#标题配置
title:  04_linRegress
#时间配置
date:   2018-05-20 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
#tag: machine_learning
---

* content
{:toc}


## Linear regression
* equation to calculate expected value:  $$ hat(y)(x) = \thetax^{T} $$
* MSE is used to quantify the error: $$ MSE = \frac{1}{m}\sum_{j}(y^{j}-\thetax^{jT})^2 = \frac_{1}{m}(y^{T}-\thetaX^{T})(y^{T}-\thetaX^{T})^{T} $$  
* Good thetas can make MSE small, the the way to find good parameters is to find the theta at the bottom of cost "surface".   

## Gradient descent&stochastic gradient descent

* To find the smallest J(\theta), we need to calculate the derivative $$ \frac{\partial J(\theta)}{\partial \theta} $$ If it is positive, J(\theta) is increasing; if it is negative, J(\theta) is decreasing. If there are multiple \theta, then imaging multi-dimension space composed of \theta, then calculate the gradient vector.   
* fake algorithm of gradient descent: 1. initialization (?); 2. set step size (\alpha, size of each move); 3. gradient direction; 4. check whether stopping condition is met after each move  
* equation to calculate gradient descent <img src="/Users/YvonneGong/Documents/2018_spring/machine_learning/screenshot/gradiant_descent.png">    

## Direct minimization  

* If we have a simple problem - trying to build a model with two data points and one feature, then the $$ \theta $$ can be calculated directly. However, there are normally many data points so that no linear function can hit all of them. Instead, we choose to solve for minimum of MSE, which is when the gradiant_descent of MSE is 0.
* derivative $$ J(\theta) = -(y^T - \theta X^T)X = 0 $$ This is called normal equation.   

## Nonlinear features   

* By increasing the number of features or add polynomial functions, the MSE of testing dataset will decrease. However, this may be resulted from overfitting. We are supposed to find a point where both training and testing MSE are at the low point.   

## Bias and variance  

* The data we get is just part of the "real-world" data. Building model based on the limited data with different polynomial degree will lead to different result. Normally high polynomial-degree model will reduce the error rate on training data but has high error rate on the real-world data, which is so called "overfitting". We need to choose the "right" complexity for models. One solution is to separate our data into two sets - training and test dataset. Learn only on training data and use test data to estimate the generalization quality and select the best model.   

* Ways to increase complexity: 1. add features, parameters; ways to decrease complexity: 1. remove features; 2. partial training, regularization     

## Regularization  

* To avoid overfitting, modify the cost function to add "preference" for certain parameter values.(?)    
$$ J(\underline{\theta}) = \frac{1}{2}(\underline{y} - \underline{\theta}\underline{X}^T)(\underline{y} - \underline{\theta}\underline{X}^T)^T _ \alpha\theta\theta^T  
Notes: regularization term is independent of the data: paying more attention reduces our model variance    

* Different regularization functions (different $$ L_p $$ regularizer): $$(\sum|\theta_i|^p)^(\frac{1}{p}) $$  
p = 1, called Lasso, which tends to generate sparser solution (fewer parameters)  
p = 2, called quadratics, the surface is a circle.

## Hold-out, cross-validation    

1. Hold-out method is to hold some data for evaluation and train only on reminder (e.g., 30/70 split). However, if we have few data, it is difficult to split data - few data in hold-out will lead to noisy estimate of error; more data in hold out leaves less fore training.   

2. Cross-validation method  
  * K-fold cross validation is divide M data points into K disjoint sets; hold out one set for evaluation and train on the other data points (= M/K * (K-1) data); then change evaluation data to another; in total needs to evaluate K times; the end model and MSE is the average  
  * Advantages: let us use more validation data  
  * Disadvantages: more work: trains K models instead of just one; don't evaluate any particular predictor   
  * More data should significantly improve performance; when data is enough the performance will saturate.   
  * There is a combination of leave one out and cross validation, called LOO XVal. The validation data is just one data point each time (K times of validation = M number of data points). Each time, all the data except for one will be used to train, and it will repeat M times (each data point held out once) and average to get the model. This requires much computational work. 
