---
layout: post
#标题配置
title:  10_clustering
#时间配置
date:   2018-06-09 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
#tag: machine_learning
---

* content
{:toc}

## Clustering

* supervised learning: predict target value ("y") given features ("x")
* unsupervised learning: understand the patterns of data (just "x"); one example is
"clustering" - describes data by "groups"

## Hierarchical agglomerative clustering

1. Initially, every point is a cluster. Then distance is calculated between clusters.
  * Initialize: every example is a cluster
  * Iterate: a. compute distances between all clusters; b. merge two closest clusters
  * This could compute dendrogram: heights of join indicates dissimilarity
  * Criteria to group: given the sequence, can select a number of clusters or dissimilarity
  threshold.

2. There are four different ways to calculate the cluster distances
    * Nearest neighbour (single linkage): $$ D_{min}(C_{i}, C_{j}) = min|x - y|^2 $$
    * Furthest neighbour (complete linkage): $$ D_{max}(C_{i}, C_{j}) = max|x - y|^2 $$
    * Average: $$ D_{avg}(C_{i}, C_{j}) = \frac_{1}{|C_{i}||C_{j}} \sum|X - y|^2 $$
    * Centroid (mean): $$ D_{mean}(C_{i}, C_{j}) = |u_{i} - u_{j}|^2 $$
    * Dissimilarity choice will affect clusters created
    <img src="{{'/styles/images/cs273a/cluster_distance.png'}}" width="100%" />

## k-Means Clustering

1. Definition: data example i has features Xi; assume K clusters; each cluster c "described"
by a center $$ u_{c} $$; each cluster will "claim" a set of nearby points; iterate between:
updating the assignment of data to clusters, then updating the cluster's summarization (the mean
of all the points in one cluster). Iterate until convergence:
  * For each datum, find the closest cluster: $$ z_{i} = arg min |x_{i} - u_{c}| ^2 $$
  * Set each cluster to the mean of all assigned data: $$ u_{c} = \frac_{1}{m_{c}} \sum x_{i} $$
  * Optimizing the cost function: $$ C(\underline{z}, \underline(u)) = \sum |z_{i} - u_{z}|^2 $$

2. Initialization: try different (randomized) initializations; use cost C to decide which we prefer.
Different initialization methods are described below:
    * Random: usually, choose random data index; insures centers are near some data; issue = may choose
    nearby points
    * Distance-based: start with one random data point; find the point farthest from the clusters chosen so
    far; issue - may choose outliers
    * Random + distance ("k-means++"): choose next points "far but randomly"; likely to put a cluster far
    away, in a region with lots of data

3. Choosing the number of clusters: cost always decreases with k increasing, since the model will be more
complex. One solution is to penalize for complexity: total cost = error + complexity penalty

## Gaussian Mixtures & EM (expectation maximization)

1. In Gaussian mixture model, each cluster is modeled as Gaussian distribution (not just by their mean). EM
 algorithm is used to assign data to cluster with some probability.
  * Mean, variance, "size": $$ u_{c}, \sigma_{c}, \pi_{x} $$
  * probability distribution: $$ p(x) = \sum \pi_{c} N(x; \mu_{c}, \sigma_{c}) $$
  * Equivalent "latent variable" form: a. $$ p(z = c) = \pi_{c} $$ Select a mixture component with probability
  $$ \pi $$; b/ $$ p(x|z = c) = N(x; \mu_{c}, \sigma_{c}) $$ Sample from that component's Gaussian

2. Multivariate Gaussian models:
  * $$ N(\underline{x}, \underline{\mu}, \sum) = \frac_{1}{(2\pi^\frac_{d}{2})}|\sum|^(-1/2)exp{-\frac_{1}{2}(\underline{x} - \underline{\mu})^T \sum^(-1)(\underline{x} - \underline{\mu})} $$

3. EM algorithm:
  * E-step: For each datum $$ x_{i} $$, compute the probability that it belongs to cluster c
    <img src="{{'/styles/images/cs273a/em_algorithm.png'}}" width="100%" />
  * M-step: Start with assignment probabilities, update the parameters for the cluster Gaussian model.
    <img src="{{'/styles/images/cs273a/em_algorithm_m_step.png'}}" width="100%" />
    
