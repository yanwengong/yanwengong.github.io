---
layout: post
#标题配置
title:  08_dectree
#时间配置
date:   2018-06-03 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
#tag: machine_learning
---

* content
{:toc}


## Decision trees

1. Decision tree is to "split" data into different cases (usually based on a single variable)
and recurse down until we reach a decision. The number of child-nodes can vary. For binary
tree, it splits values into two sets. The complexity of function depends on the depth. A depth-1
decision tree is called a decision "stump"， which is simpler than a linear classifier.

## Learning decision trees

1. Leaf node is the last node, which can make prediction given this data subset. If is to classify,
pick majority class, If it's regress, predict average value. If it's non-leaf nodes, pick a feature
and split.

2. “Entropy” is a measurement of randomness. It takes less work to communicate if the result is less
random.
  * Entropy $$ H(x) = E[ log \frac{1}{p(x)}] = \sum p(x)log\frac{1}{p(x)} $$
  * log base two, units of entropy are "bits"

3. Information gain is how much is entropy reduced by splitting
  * Information = prob_of_A_in_A_group * (H(before) - H(in_group_A)) + prob_of_B_in_B_group * (H(before) - H(in_group_B))
  * example is below <img src="{{'/styles/images/cs273a/entropy_and_information.png'}}" width="100%" />
  * An alternative to information is to measure variance in the allocation (instead of entropy), which is
  nearly the same

3. The pseudo-code to build a decision tree
<img src="{{'/styles/images/cs273a/pseudo-code_decision_tree.png'}}" width="100%" />
  * Please note: use information gain threshold as one of the stopping criteria is often not a good idea. Sometimes one
  single split cannot improve performance but two splits together is accurate. The better way is to grow a large tree and
  prune back, using training or validation data.
  * Ways to control complexity: a. set minimum # of parent data; b. set maximum depth cutoff   
