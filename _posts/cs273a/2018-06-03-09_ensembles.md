---
layout: post
#标题配置
title:  09_ensembles
#时间配置
date:   2018-06-03 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
#tag: machine_learning
---

* content
{:toc}


## Ensembles of learners

1. Ensemble  is combining many predictors, which can be weighted or un-weighted.
  * The un-weighted method is to take the average of predicted result or the majority vote
  * The weighted methods is to up-weight "better" predictor. Ex: classes (+1, -1) =>
  $$ \hat{y}_e = sign(\sum \alpha_i \hat{y}_i) $$

2. Another option of ensemble is to train a "predictor of predictors"
  * treat individual predictors as features, similar to multi-layer perceptron idea

## Bagging

1. "baggings" = bootstrap aggregation:
  * bootstrap: create a random subset of data by sampling; draw m' of the m samples,
  with replacement (somtimes w/o)
  * create a training set of m' <= m samples, train a classifier on the random training
  set; repeat k times
  * to test, run each trained classifier; each classifier votes on the output, take majority;
  for regression, take the average of each regressor's predicted output
  learn many classifiers, each with only part of the data
  * combine through model averaging

2. bagged decision trees
  * randomly resample data, learn a decision tree for each
  * to reduce memorization effect: a. not every predictor sees each data point; b. lowers
  "complexity" of the overall average; c. usually, better generalization performance

3. random forests
