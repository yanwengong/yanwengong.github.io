---
layout: post
#标题配置
title:  11_PCA&SVD
#时间配置
date:   2018-06-10 01:08:00 +0800
#大类配置
categories: study_notes
#小类配置
#tag: machine_learning
---

* content
{:toc}

## PCA and SVD

1. Demensionality reduction
  * want to communicate a "model" to convert data with two real values [x1, x2] to only one
  value [z1]: [x1, x2] ~ f(z1)
  * Ex: linear function f(z): $$[x_{1}, x_{2}]= z*\underline{v} = z*[v_{1}, v_{2}] \underline{v} $$ is
  the same for all data points, z tells us the closest point on v to the original point [x1, x2]
  <img src="{{'/styles/images/cs273a/dimentionality_reduction.png'}}" width="100%" />

2. Principal component analysis (PCA): want the the vector that would most closely reconstruct X?
$$ min\sum(x^(i) - \alpha^(i)v)^2 $$
  * given c: $$ a^(i) $$ is the projection of each point $$ x^(i) onto v
  * v chosen to minimize the residual variance; equivalently, v is the direction of maximum variance

TO BE CONTINUED
